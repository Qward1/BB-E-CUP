{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "EDA для текстовых данных",
   "id": "2ee8865c46f5b3e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# notebooks/01_eda_large_dataset.ipynb\n",
    "\"\"\"\n",
    "EDA для больших датасетов - пошаговый план\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LargeScaleEDA:\n",
    "    \"\"\"EDA для больших датасетов с использованием выборок и Dask\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, sample_size: int = 100000):\n",
    "        self.data_path = data_path\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def step1_basic_info(self):\n",
    "        \"\"\"Шаг 1: Базовая информация о датасете\"\"\"\n",
    "        print(\"=\"*50)\n",
    "        print(\"STEP 1: BASIC DATASET INFO\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Используем Dask для метаинформации\n",
    "        if self.data_path.endswith('.parquet'):\n",
    "            df = dd.read_parquet(self.data_path)\n",
    "        else:\n",
    "            df = dd.read_csv(self.data_path, blocksize='64MB')\n",
    "\n",
    "        print(f\"Total rows: {len(df):,}\")\n",
    "        print(f\"Total columns: {len(df.columns)}\")\n",
    "        print(f\"Estimated memory: {df.memory_usage(deep=True).sum().compute() / 1e9:.2f} GB\")\n",
    "        print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "        print(f\"\\nDtypes:\\n{df.dtypes}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def step2_sampling_strategy(self):\n",
    "        \"\"\"Шаг 2: Стратегии выборки\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 2: SAMPLING STRATEGIES\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        strategies = {\n",
    "            'random': self._random_sample,\n",
    "            'stratified': self._stratified_sample,\n",
    "            'systematic': self._systematic_sample,\n",
    "            'cluster': self._cluster_sample\n",
    "        }\n",
    "\n",
    "        samples = {}\n",
    "        for name, method in strategies.items():\n",
    "            print(f\"\\nCreating {name} sample...\")\n",
    "            samples[name] = method()\n",
    "            print(f\"  Sample size: {len(samples[name])}\")\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def _random_sample(self) -> pd.DataFrame:\n",
    "        \"\"\"Случайная выборка\"\"\"\n",
    "        # Читаем random строки\n",
    "        n_rows = sum(1 for _ in open(self.data_path.replace('.parquet', '.csv')))\n",
    "        skip_rows = np.random.choice(n_rows, n_rows - self.sample_size, replace=False)\n",
    "\n",
    "        if self.data_path.endswith('.parquet'):\n",
    "            df = pd.read_parquet(self.data_path)\n",
    "            return df.sample(n=min(self.sample_size, len(df)))\n",
    "        else:\n",
    "            return pd.read_csv(self.data_path, skiprows=skip_rows, nrows=self.sample_size)\n",
    "\n",
    "    def _stratified_sample(self) -> pd.DataFrame:\n",
    "        \"\"\"Стратифицированная выборка по целевой переменной\"\"\"\n",
    "        # Читаем только целевую переменную и индексы\n",
    "        if self.data_path.endswith('.parquet'):\n",
    "            df = pd.read_parquet(self.data_path, columns=['is_counterfeit'])\n",
    "        else:\n",
    "            df = pd.read_csv(self.data_path, usecols=['is_counterfeit'])\n",
    "\n",
    "        # Стратифицированная выборка\n",
    "        sample_idx = df.groupby('is_counterfeit').apply(\n",
    "            lambda x: x.sample(n=min(len(x), self.sample_size//2))\n",
    "        ).index.get_level_values(1)\n",
    "\n",
    "        # Читаем полные данные для выбранных индексов\n",
    "        if self.data_path.endswith('.parquet'):\n",
    "            return pd.read_parquet(self.data_path).iloc[sample_idx]\n",
    "        else:\n",
    "            return pd.read_csv(self.data_path, skiprows=lambda x: x not in sample_idx)\n",
    "\n",
    "    def _systematic_sample(self) -> pd.DataFrame:\n",
    "        \"\"\"Систематическая выборка (каждая k-я строка)\"\"\"\n",
    "        if self.data_path.endswith('.parquet'):\n",
    "            df = pd.read_parquet(self.data_path)\n",
    "            k = len(df) // self.sample_size\n",
    "            return df.iloc[::k][:self.sample_size]\n",
    "        else:\n",
    "            # Для CSV читаем каждую k-ю строку\n",
    "            k = sum(1 for _ in open(self.data_path)) // self.sample_size\n",
    "            return pd.read_csv(self.data_path, skiprows=lambda i: i % k != 0, nrows=self.sample_size)\n",
    "\n",
    "    def _cluster_sample(self) -> pd.DataFrame:\n",
    "        \"\"\"Кластерная выборка (по категориям/брендам)\"\"\"\n",
    "        # Выбираем случайные категории/бренды\n",
    "        if self.data_path.endswith('.parquet'):\n",
    "            df = pd.read_parquet(self.data_path, columns=['category'])\n",
    "            selected_categories = df['category'].value_counts().head(10).index\n",
    "            df_full = pd.read_parquet(self.data_path)\n",
    "            return df_full[df_full['category'].isin(selected_categories)].sample(\n",
    "                n=min(self.sample_size, len(df_full))\n",
    "            )\n",
    "\n",
    "    def step3_missing_analysis(self, df_sample: pd.DataFrame):\n",
    "        \"\"\"Шаг 3: Анализ пропусков\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 3: MISSING DATA ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        missing_stats = pd.DataFrame({\n",
    "            'column': df_sample.columns,\n",
    "            'missing_count': df_sample.isnull().sum(),\n",
    "            'missing_percent': df_sample.isnull().sum() / len(df_sample) * 100\n",
    "        }).sort_values('missing_percent', ascending=False)\n",
    "\n",
    "        print(\"\\nColumns with missing values:\")\n",
    "        print(missing_stats[missing_stats['missing_count'] > 0])\n",
    "\n",
    "        # Паттерны пропусков\n",
    "        print(\"\\nMissing patterns:\")\n",
    "        missing_patterns = df_sample.isnull().value_counts()\n",
    "        print(missing_patterns.head(10))\n",
    "\n",
    "        return missing_stats\n",
    "\n",
    "    def step4_distribution_analysis(self, df_sample: pd.DataFrame):\n",
    "        \"\"\"Шаг 4: Анализ распределений\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 4: DISTRIBUTION ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Числовые переменные\n",
    "        numeric_cols = df_sample.select_dtypes(include=[np.number]).columns\n",
    "        print(f\"\\nNumeric columns ({len(numeric_cols)}):\")\n",
    "        print(df_sample[numeric_cols].describe())\n",
    "\n",
    "        # Категориальные переменные\n",
    "        categorical_cols = df_sample.select_dtypes(include=['object', 'category']).columns\n",
    "        print(f\"\\nCategorical columns ({len(categorical_cols)}):\")\n",
    "        for col in categorical_cols[:5]:  # Первые 5 для примера\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(df_sample[col].value_counts().head(10))\n",
    "\n",
    "    def step5_text_analysis(self, df_sample: pd.DataFrame):\n",
    "        \"\"\"Шаг 5: Анализ текстовых данных\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 5: TEXT ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        text_columns = ['title', 'description']\n",
    "\n",
    "        for col in text_columns:\n",
    "            if col in df_sample.columns:\n",
    "                print(f\"\\n{col} analysis:\")\n",
    "\n",
    "                # Длина текстов\n",
    "                text_lengths = df_sample[col].fillna('').str.len()\n",
    "                print(f\"  Length stats: mean={text_lengths.mean():.0f}, \"\n",
    "                      f\"median={text_lengths.median():.0f}, \"\n",
    "                      f\"max={text_lengths.max():.0f}\")\n",
    "\n",
    "                # Количество слов\n",
    "                word_counts = df_sample[col].fillna('').str.split().str.len()\n",
    "                print(f\"  Word count: mean={word_counts.mean():.0f}, \"\n",
    "                      f\"median={word_counts.median():.0f}\")\n",
    "\n",
    "                # Процент заполненности\n",
    "                filled_percent = (df_sample[col].notna().sum() / len(df_sample)) * 100\n",
    "                print(f\"  Filled: {filled_percent:.1f}%\")\n",
    "\n",
    "    def step6_target_analysis(self, df_sample: pd.DataFrame):\n",
    "        \"\"\"Шаг 6: Анализ целевой переменной\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 6: TARGET VARIABLE ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        target = 'is_counterfeit'\n",
    "        if target in df_sample.columns:\n",
    "            print(f\"\\nTarget distribution:\")\n",
    "            print(df_sample[target].value_counts(normalize=True))\n",
    "\n",
    "            # Анализ по категориям\n",
    "            if 'category' in df_sample.columns:\n",
    "                print(f\"\\nCounterfeit rate by category:\")\n",
    "                counterfeit_by_category = df_sample.groupby('category')[target].mean().sort_values(ascending=False)\n",
    "                print(counterfeit_by_category.head(10))\n",
    "\n",
    "            # Анализ по брендам\n",
    "            if 'brand' in df_sample.columns:\n",
    "                print(f\"\\nCounterfeit rate by brand:\")\n",
    "                counterfeit_by_brand = df_sample.groupby('brand')[target].mean().sort_values(ascending=False)\n",
    "                print(counterfeit_by_brand.head(10))\n",
    "\n",
    "    def step7_correlation_analysis(self, df_sample: pd.DataFrame):\n",
    "        \"\"\"Шаг 7: Корреляционный анализ\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 7: CORRELATION ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Только числовые колонки\n",
    "        numeric_df = df_sample.select_dtypes(include=[np.number])\n",
    "\n",
    "        if 'is_counterfeit' in numeric_df.columns:\n",
    "            correlations = numeric_df.corr()['is_counterfeit'].sort_values(ascending=False)\n",
    "            print(\"\\nTop correlations with target:\")\n",
    "            print(correlations.head(10))\n",
    "            print(\"\\nBottom correlations with target:\")\n",
    "            print(correlations.tail(10))\n",
    "\n",
    "    def step8_anomaly_detection(self, df_sample: pd.DataFrame):\n",
    "        \"\"\"Шаг 8: Поиск аномалий\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STEP 8: ANOMALY DETECTION\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Поиск выбросов в цене\n",
    "        if 'price' in df_sample.columns:\n",
    "            Q1 = df_sample['price'].quantile(0.25)\n",
    "            Q3 = df_sample['price'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            outliers = df_sample[(df_sample['price'] < Q1 - 1.5*IQR) |\n",
    "                                 (df_sample['price'] > Q3 + 1.5*IQR)]\n",
    "\n",
    "            print(f\"\\nPrice outliers: {len(outliers)} ({len(outliers)/len(df_sample)*100:.2f}%)\")\n",
    "            print(f\"Outlier price range: {outliers['price'].min():.2f} - {outliers['price'].max():.2f}\")\n",
    "\n",
    "            # Проверяем, чаще ли outliers являются контрафактом\n",
    "            if 'is_counterfeit' in df_sample.columns:\n",
    "                outlier_counterfeit_rate = outliers['is_counterfeit'].mean()\n",
    "                normal_counterfeit_rate = df_sample[~df_sample.index.isin(outliers.index)]['is_counterfeit'].mean()\n",
    "                print(f\"Counterfeit rate in outliers: {outlier_counterfeit_rate:.2%}\")\n",
    "                print(f\"Counterfeit rate in normal: {normal_counterfeit_rate:.2%}\")\n",
    "\n",
    "    def run_complete_eda(self):\n",
    "        \"\"\"Запуск полного EDA\"\"\"\n",
    "        # Шаг 1: Базовая информация\n",
    "        dask_df = self.step1_basic_info()\n",
    "\n",
    "        # Шаг 2: Создание выборок\n",
    "        samples = self.step2_sampling_strategy()\n",
    "\n",
    "        # Используем random sample для дальнейшего анализа\n",
    "        df_sample = samples['random']\n",
    "\n",
    "        # Шаги 3-8: Детальный анализ\n",
    "        self.step3_missing_analysis(df_sample)\n",
    "        self.step4_distribution_analysis(df_sample)\n",
    "        self.step5_text_analysis(df_sample)\n",
    "        self.step6_target_analysis(df_sample)\n",
    "        self.step7_correlation_analysis(df_sample)\n",
    "        self.step8_anomaly_detection(df_sample)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EDA COMPLETE!\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        return df_sample\n",
    "\n",
    "# Использование\n",
    "eda = LargeScaleEDA('data/train.parquet', sample_size=100000)\n",
    "df_sample = eda.run_complete_eda()"
   ],
   "id": "b593ac38b24266af"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
