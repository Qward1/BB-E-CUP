"""
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è e-commerce –æ–ø–∏—Å–∞–Ω–∏–π
"""

import re
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Set
from collections import Counter
import Levenshtein


class EcommerceFeatureExtractor:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è e-commerce –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """

    def __init__(self):
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞

        self.price_pattern = re.compile(r'\d+\s*(?:—Ä—É–±|—Ä—É–±–ª|‚ÇΩ|—Ä\.|dollars?|\$|–µ–≤—Ä–æ|‚Ç¨)')

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        self.copy_keywords = [
            '–∫–æ–ø–∏—è', '–∫–æ–ø–∏—é', '–∞–Ω–∞–ª–æ–≥', '–∑–∞–º–µ–Ω–∏—Ç–µ–ª—å', '—Å–æ–≤–º–µ—Å—Ç–∏–º',
            '–ø–æ–¥—Ö–æ–¥–∏—Ç', '–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞', 'replica', 'copy', 'compatible',
            'replacement', 'substitute', '–∫–∞–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª', '—Ç–æ—á–Ω–∞—è –∫–æ–ø–∏—è'
        ]



    def extract_features(self, text: str, brand_name: Optional[str] = None,
                         category: Optional[str] = None) -> Dict[str, float]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è

        Args:
            text: –û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
            brand_name: –ë—Ä–µ–Ω–¥ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        features = {}

        # –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        text_clean = self._clean_html(text)
        text_lower = text_clean.lower()

        # 1. HTML –∞–Ω–∞–ª–∏–∑
        features.update(self._extract_html_features(text))

        # 2. –ú–æ–¥–µ–ª–∏ –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å


        # 3. –†–∞–∑–º–µ—Ä—ã –∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏


        # 4. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        features.update(self._extract_keyword_features(text_lower))

        # 5. –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
        features.update(self._extract_style_features(text_clean))

        # 6. –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è


        # 7. –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –∏ –∞–Ω–æ–º–∞–ª–∏–∏
        features.update(self._extract_repetition_features(text_clean))

        return features

    def _clean_html(self, text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤"""
        return re.sub(r'<[^>]+>', ' ', str(text))

    def _extract_html_features(self, text: str) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Ñ–∞–∫—Ç–∞"""
        features = {}

        # –ë–∞–∑–æ–≤—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —Ç–µ–≥–∏
        br_tags = len(re.findall(r'<br\s*/?>', str(text), re.IGNORECASE))
        p_tags = len(re.findall(r'<p[^>]*>', str(text), re.IGNORECASE))
        div_tags = len(re.findall(r'<div[^>]*>', str(text), re.IGNORECASE))
        span_tags = len(re.findall(r'<span[^>]*>', str(text), re.IGNORECASE))

        # –¢–µ–≥–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–∫–æ–Ω—Ç—Ä–∞—Ñ–∞–∫—Ç —á–∞—Å—Ç–æ –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç)
        strong_tags = len(re.findall(r'<(strong|b)\b[^>]*>', str(text), re.IGNORECASE))
        italic_tags = len(re.findall(r'<(em|i)\b[^>]*>', str(text), re.IGNORECASE))
        underline_tags = len(re.findall(r'<u\b[^>]*>', str(text), re.IGNORECASE))

        # –¶–≤–µ—Ç–æ–≤—ã–µ –∏ —Å—Ç–∏–ª–µ–≤—ã–µ —Ç–µ–≥–∏ (—á–∞—Å—Ç–æ –≤ –∫–æ–Ω—Ç—Ä–∞—Ñ–∞–∫—Ç–µ –¥–ª—è –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è)
        font_tags = len(re.findall(r'<font[^>]*>', str(text), re.IGNORECASE))
        color_attrs = len(re.findall(r'(?:color|style)\s*=\s*["\'][^"\']*(?:red|green|blue|yellow|#[0-9a-f]{6})', str(text),
                                     re.IGNORECASE))
        style_attrs = len(re.findall(r'style\s*=\s*["\'][^"\']*["\']', str(text), re.IGNORECASE))

        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ (H1-H6)
        h_tags = len(re.findall(r'<h[1-6]\b[^>]*>', str(text), re.IGNORECASE))
        h1_tags = len(re.findall(r'<h1\b[^>]*>', str(text), re.IGNORECASE))  # H1 –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏





        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã HTML entities
        html_entities = len(re.findall(r'&(?:#\d+|#x[0-9a-f]+|\w+);', str(text), re.IGNORECASE))
        nbsp_count = str(text).count('&nbsp;')

        # –°–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏ (–æ—á–µ–Ω—å –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ)
        script_tags = len(re.findall(r'<script\b[^>]*>', str(text), re.IGNORECASE))
        style_tags = len(re.findall(r'<style\b[^>]*>', str(text), re.IGNORECASE))

        # –ù–µ–≤–∞–ª–∏–¥–Ω—ã–µ/—Å–ª–æ–º–∞–Ω–Ω—ã–µ —Ç–µ–≥–∏ (–ø—Ä–∏–∑–Ω–∞–∫ –∫–æ–ø–∏–ø–∞—Å—Ç–∞)
        broken_tags = len(re.findall(r'<[^>]*<|>[^<]*>', str(text)))  # –ù–µ–∑–∞–∫—Ä—ã—Ç—ã–µ –∏–ª–∏ –¥–≤–æ–π–Ω—ã–µ
        mismatched_tags = self._count_mismatched_tags(str(text))

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features['has_html_tags'] = 1 if '<' in str(text) else 0
        features['html_total_tags'] = sum([
            br_tags, p_tags, div_tags, span_tags, strong_tags, italic_tags,
            underline_tags, font_tags, h_tags,

        ])

        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —Ç–µ–≥–∏
        features['html_br_count'] = br_tags
        features['html_p_count'] = p_tags
        features['html_div_count'] = div_tags
        features['html_span_count'] = span_tags

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–∫–æ–Ω—Ç—Ä–∞—Ñ–∞–∫—Ç —á–∞—Å—Ç–æ –ø–µ—Ä–µ—É—Å–µ—Ä–¥—Å—Ç–≤—É–µ—Ç)
        features['html_formatting_tags'] = strong_tags + italic_tags + underline_tags
        features['html_strong_tags'] = strong_tags
        features['html_italic_tags'] = italic_tags
        features['html_underline_tags'] = underline_tags
        features['excessive_formatting'] = 1 if features['html_formatting_tags'] > 5 else 0

        # –°—Ç–∏–ª–∏–∑–∞—Ü–∏—è (–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ)
        features['html_font_tags'] = font_tags  # –£—Å—Ç–∞—Ä–µ–≤—à–∏–π —Ç–µ–≥
        features['html_color_usage'] = color_attrs
        features['html_style_attrs'] = style_attrs
        features['has_colored_text'] = 1 if color_attrs > 0 else 0

        # –ó–∞–≥–æ–ª–æ–≤–∫–∏
        features['html_header_tags'] = h_tags
        features['has_h1_tag'] = 1 if h1_tags > 0 else 0  # H1 –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ

        # HTML entities
        features['html_entities_count'] = html_entities
        features['html_nbsp_count'] = nbsp_count
        features['excessive_nbsp'] = 1 if nbsp_count > 10 else 0

        # –ö–∞—á–µ—Å—Ç–≤–æ HTML
        features['html_broken_tags'] = broken_tags
        features['html_mismatched_tags'] = mismatched_tags
        features['poor_html_quality'] = 1 if (broken_tags + mismatched_tags) > 0 else 0

        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
        text_clean = self._clean_html(text)
        features['html_to_text_ratio'] = 1 - (len(text_clean) / max(len(str(text)), 1))
        features['formatting_density'] = features['html_formatting_tags'] / max(len(str(text).split()), 1)

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∫–æ–Ω—Ç—Ä–∞—Ñ–∞–∫—Ç–∞
        features['html_complexity_score'] = self._calculate_html_complexity(features)
        features['html_suspicious_pattern'] = self._detect_suspicious_html_pattern(text)

        return features

    def _count_mismatched_tags(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏—Ö –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö —Ç–µ–≥–æ–≤"""
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–∞—Ä–Ω–æ—Å—Ç—å –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–≥–æ–≤
        mismatched = 0
        tags_to_check = ['p', 'div', 'span', 'strong', 'b', 'em', 'i', 'u', 'ul', 'ol', 'li']

        for tag in tags_to_check:
            opening = len(re.findall(f'<{tag}\\b[^>]*>', str(text), re.IGNORECASE))
            closing = len(re.findall(f'</{tag}>', str(text), re.IGNORECASE))
            if opening != closing:
                mismatched += abs(opening - closing)

        return mismatched

    def _calculate_html_complexity(self, features: Dict[str, float]) -> float:
        """
        –†–∞—Å—á–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        –°–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–∞—è –∏–ª–∏ —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –∫–æ–Ω—Ç—Ä–∞—Ñ–∞–∫—Ç
        """
        complexity = 0

        # –§–∞–∫—Ç–æ—Ä—ã —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        complexity += features.get('html_total_tags', 0) * 0.1
        complexity += features.get('html_table_complexity', 0) * 0.5
        complexity += features.get('html_list_tags', 0) * 0.3
        complexity += features.get('html_style_attrs', 0) * 0.2

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç 0 –¥–æ 10
        return min(complexity, 10)

    def _detect_suspicious_html_pattern(self, text: str) -> int:
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö HTML –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        """
        suspicious_score = 0

        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ <br/> –ø–æ–¥—Ä—è–¥ (—á–∞—Å—Ç–æ –≤ –∫–æ–Ω—Ç—Ä–∞—Ñ–∞–∫—Ç–µ)
        if re.search(r'(?:<br\s*/?>[\s]*){3,}', str(text), re.IGNORECASE):
            suspicious_score += 1

        # –ß—Ä–µ–∑–º–µ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ bold/strong
        if re.search(r'(?:<(?:strong|b)>.*?</(?:strong|b)>[\s]*){3,}', str(text), re.IGNORECASE):
            suspicious_score += 1

        # Inline —Å—Ç–∏–ª–∏ —Å —è—Ä–∫–∏–º–∏ —Ü–≤–µ—Ç–∞–º–∏ (red, yellow, green)
        if re.search(r'style\s*=\s*["\'][^"\']*(?:red|yellow|#ff0000|#ffff00|#00ff00)', str(text), re.IGNORECASE):
            suspicious_score += 1

        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–∫–∏ –≤ HTML
        if re.search(r'<[^>]*>.*?!!!+.*?</[^>]*>', str(text)):
            suspicious_score += 1

        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Ç–µ–≥–æ–≤
        deprecated_tags = ['font', 'center', 'marquee', 'blink']
        for tag in deprecated_tags:
            if re.search(f'<{tag}\\b', str(text), re.IGNORECASE):
                suspicious_score += 1

        # –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç–∏–ª–∏ –∏–∑ Word/–¥—Ä—É–≥–∏—Ö —Ä–µ–¥–∞–∫—Ç–æ—Ä–æ–≤
        if 'mso-' in str(text) or 'MsoNormal' in str(text):
            suspicious_score += 2  # –Ø–≤–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –∫–æ–ø–∏–ø–∞—Å—Ç–∞

        # –ü—É—Å—Ç—ã–µ —Ç–µ–≥–∏
        if re.search(r'<(?:p|div|span|strong|b|em|i)(?:\s[^>]*)?>[\s]*</(?:p|div|span|strong|b|em|i)>', str(text),
                     re.IGNORECASE):
            suspicious_score += 1

        return suspicious_score



    def _extract_keyword_features(self, text: str) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        features = {}

        # –°–ª–æ–≤–∞-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∫–æ–ø–∏–π
        copy_score = sum(1 for keyword in self.copy_keywords if keyword in str(text))
        features['copy_words_count'] = copy_score
        features['has_copy_words'] = 1 if copy_score > 0 else 0

        # –£–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Ü–µ–Ω—ã
        prices = self.price_pattern.findall(str(text))
        features['price_mentioned'] = 1 if prices else 0
        features['price_mentions_count'] = len(prices)

        # –ü—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é
        cta_phrases = ['–∫—É–ø–∏', '–∑–∞–∫–∞–∂', '—Å–ø–µ—à–∏', '—Å–∫–∏–¥–∫', '–∞–∫—Ü–∏', 'buy', 'order', 'hurry', 'discount']
        features['call_to_action_count'] = sum(1 for phrase in cta_phrases if phrase in str(text))

        return features

    def _extract_style_features(self, text: str) -> Dict[str, float]:
        """–°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
        –ù–£–ñ–ù–û –î–û–†–ê–ë–û–¢–ê–¢–¨ –ê–ù–ê–õ–ò–ó –ê–ë–ó–ê–¶–ï–í"""
        features = {}

        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–≥–∏—Å—Ç—Ä–∞
        if text:
            upper_chars = sum(1 for c in str(text) if c.isupper())
            lower_chars = sum(1 for c in str(text) if c.islower())
            total_alpha = upper_chars + lower_chars

            features['uppercase_ratio'] = upper_chars / max(total_alpha, 1)
            features['all_uppercase'] = 1 if upper_chars > 0 and lower_chars == 0 else 0
            features['all_lowercase'] = 1 if lower_chars > 0 and upper_chars == 0 else 0

            # –ê–Ω–æ–º–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
            words = str(text).split()
            capitalized_words = sum(1 for w in words if w and w[0].isupper())
            features['capitalized_words_ratio'] = capitalized_words / max(len(words), 1)

        # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
        punctuation = '!?.,:;-‚Äî'
        punct_count = sum(1 for c in str(text) if c in punctuation)
        features['punctuation_density'] = punct_count / max(len(str(text)), 1)
        features['exclamation_marks'] = str(text).count('!')
        features['question_marks'] = str(text).count('?')
        features['ellipsis_count'] = str(text).count('...')

        # –≠–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
        emoji_pattern = re.compile(r'[üòÄ-üôèüåÄ-üóøüöÄ-üõøüèÄ-üèø]')
        features['has_emoji'] = 1 if emoji_pattern.search(str(text)) else 0

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–±–∑–∞—Ü–µ–≤
        paragraphs = str(text).split('\n\n')
        features['paragraph_count'] = len(paragraphs)
        features['avg_paragraph_length'] = np.mean([len(p) for p in paragraphs]) if paragraphs else 0

        return features




    def _extract_repetition_features(self, text: str) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        features = {}

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = str(text).lower().split()

        if words:
            # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤
            unique_words = set(words)
            features['word_uniqueness'] = len(unique_words) / len(words)

            # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞
            word_counts = Counter(words)
            repeated_words = [w for w, c in word_counts.items() if c > 2]
            features['repeated_words_count'] = len(repeated_words)

            # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –±–∏–≥—Ä–∞–º–º—ã
            bigrams = [f"{words[i]} {words[i + 1]}" for i in range(len(words) - 1)]
            bigram_counts = Counter(bigrams)
            repeated_bigrams = [b for b, c in bigram_counts.items() if c > 1]
            features['repeated_bigrams_count'] = len(repeated_bigrams)

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞
            max_repetition = max(word_counts.values()) if word_counts else 0
            features['max_word_repetition'] = max_repetition

            # –≠–Ω—Ç—Ä–æ–ø–∏—è —Ç–µ–∫—Å—Ç–∞ (–º–µ—Ä–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è)
            total_words = len(words)
            entropy = -sum((count / total_words) * np.log2(count / total_words)
                           for count in word_counts.values())
            features['text_entropy'] = entropy

        # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]', str(text))
        sentence_counts = Counter(s.strip() for s in sentences if s.strip())
        repeated_sentences = sum(1 for c in sentence_counts.values() if c > 1)
        features['repeated_sentences'] = repeated_sentences

        return features